{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272517fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunking codes \n",
    "#Chunking is an NLP technique which is used to group words or tokens into phrases in order to analyze the structure\n",
    "#of a sentence .This grouping includes pos tags as well as pharses from a sentence .ipynb_checkpoints/q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6130dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noun Pharse --NP\n",
    "#Verb Pharse --VP\n",
    "#Adjective Pharse --ADJP\n",
    "#Preposition Pharse --PP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ff7831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'crazy', 'brown', 'dog', 'went', 'running', 'through', 'the', 'mud', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('crazy', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('dog', 'NN'),\n",
       " ('went', 'VBD'),\n",
       " ('running', 'VBG'),\n",
       " ('through', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mud', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to chunk nu=oun pharses (NP) from a text\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sample =\"\"\" The crazy brown dog went running through the mud .\"\"\"\n",
    "tokenize_text=nltk.word_tokenize(sample)\n",
    "print(tokenize_text)\n",
    "pos_text=nltk.pos_tag(tokenize_text)\n",
    "pos_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98456dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT crazy/JJ brown/NN dog/NN)\n",
      "  went/VBD\n",
      "  running/VBG\n",
      "  through/IN\n",
      "  (NP the/DT mud/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#To chunk noun pharses from a text \n",
    "grammer = (r'''\n",
    "NP : {<DT>?<JJ>*<NN>*}\n",
    "           ''' )\n",
    "chunkParser=nltk.RegexpParser(grammer)\n",
    "tree = chunkParser.parse(pos_text)\n",
    "print(tree)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11794d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'The', 'term', '5G', 'refers', 'to', 'the', 'fifth', 'generation', 'of', 'mobile', 'technologhy', 'which', 'promises', 'of', 'faster', 'browsing', 'streaming', ',', 'and', 'download', 'speed', ',', 'as', 'well', 'as', 'better', 'connectivity', '.']\n",
      "(S\n",
      "  ``/``\n",
      "  (NP The/DT term/NN)\n",
      "  5G/CD\n",
      "  refers/NNS\n",
      "  to/TO\n",
      "  (NP the/DT fifth/JJ generation/NN)\n",
      "  of/IN\n",
      "  (NP mobile/JJ technologhy/NN)\n",
      "  which/WDT\n",
      "  promises/VBZ\n",
      "  of/IN\n",
      "  (NP faster/NN)\n",
      "  browsing/VBG\n",
      "  (NP streaming/NN)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (NP download/NN)\n",
      "  (NP speed/NN)\n",
      "  ,/,\n",
      "  as/RB\n",
      "  well/RB\n",
      "  as/IN\n",
      "  better/JJR\n",
      "  (NP connectivity/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#To chunk Regualar expression from a text\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sample=\"\"\"\"The term 5G refers to the fifth generation of mobile technologhy  which promises of faster browsing streaming , and download speed ,as well as better connectivity.\"\"\"\n",
    "tokenize_text=nltk.word_tokenize(sample)\n",
    "print(tokenize_text)\n",
    "\n",
    "#to attach pos tags as text \n",
    "text = nltk.pos_tag(tokenize_text)\n",
    "text\n",
    "\n",
    "#To define a grammer using Regular Expression \n",
    "grammer=r\"\"\"\n",
    "     NP:{<DT|PP\\S>?<JJ>*<NN>} # tO Chunk determines/possesive ,adjective and noun \n",
    "     {<NNP>+}                 # chunk sequences of proper nouns \n",
    "     \"\"\"\n",
    "\n",
    "chunkParser=nltk.RegexpParser(grammer)\n",
    "tree = chunkParser.parse(text)\n",
    "print(tree)\n",
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99967bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S He/PRP should/MD wait/VB before/IN going/VBG (NP swimming/NN))\n"
     ]
    }
   ],
   "source": [
    "#TO chunk verb pharses \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sample=\"\"\"He should wait before going swimming \"\"\"\n",
    "pos_text=nltk.pos_tag(nltk.word_tokenize(sample))\n",
    "pos_text\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN>}   # Chunk determiners, adjectives, and nouns\n",
    "    NP: {<NNP>+}                # Chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "chunkparser=nltk.RegexpParser(grammar)\n",
    "tree= chunkparser.parse(pos_text)\n",
    "print(tree)\n",
    "\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP The/DT dark/JJ cloud/NN) covered/VBD (NP the/DT sky/NN))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#examples  1\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sample=\"The dark cloud covered the sky \"\n",
    "pos=nltk.pos_tag(word_tokenize(sample))\n",
    "pos\n",
    "regex =r\"\"\"\n",
    "       NP : {<DT>?<JJ>*<NN>*}\n",
    "\"\"\"\n",
    "parser= nltk.RegexpParser(regex)\n",
    "tree = parser.parse(pos)\n",
    "print(tree)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e73e465",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Illegal chunk pattern: {<P><NP>>}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ramakrishna\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\regexp.py:380\u001b[0m, in \u001b[0;36mRegexpChunkRule.fromstring\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rule[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m rule[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChunkRule(rule[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], comment)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m rule[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m rule[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Ramakrishna\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\regexp.py:423\u001b[0m, in \u001b[0;36mChunkRule.__init__\u001b[1;34m(self, tag_pattern, descr)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pattern \u001b[38;5;241m=\u001b[39m tag_pattern\n\u001b[0;32m    421\u001b[0m regexp \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?P<chunk>\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;241m%\u001b[39m (tag_pattern2re_pattern(tag_pattern), ChunkString\u001b[38;5;241m.\u001b[39mIN_STRIP_PATTERN)\n\u001b[0;32m    424\u001b[0m )\n\u001b[0;32m    425\u001b[0m RegexpChunkRule\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, regexp, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mg<chunk>}\u001b[39m\u001b[38;5;124m\"\u001b[39m, descr)\n",
      "File \u001b[1;32mc:\\Users\\Ramakrishna\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\regexp.py:942\u001b[0m, in \u001b[0;36mtag_pattern2re_pattern\u001b[1;34m(tag_pattern)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m CHUNK_TAG_PATTERN\u001b[38;5;241m.\u001b[39mmatch(tag_pattern):\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad tag pattern: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m tag_pattern)\n\u001b[0;32m    944\u001b[0m \u001b[38;5;66;03m# Replace \".\" with CHUNK_TAG_CHAR.\u001b[39;00m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# We have to do this after, since it adds {}[]<>s, which would\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;66;03m# confuse CHUNK_TAG_PATTERN.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;66;03m# PRE doesn't have lookback assertions, so reverse twice, and do\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# the pattern backwards (with lookahead assertions).  This can be\u001b[39;00m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# made much cleaner once we can switch back to SRE.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Bad tag pattern: '(<(P)>)(<(NP)>))>)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m      6\u001b[0m pos\n\u001b[0;32m      7\u001b[0m regx\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m     NP: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m<DT>?<JJ>*<NN>}\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m     P: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m<IN>}\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m     VP: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m<VB.*><NP|PP>*}\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m parser\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mRegexpParser(regx)\n\u001b[0;32m     15\u001b[0m tree\u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(pos)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(tree)\n",
      "File \u001b[1;32mc:\\Users\\Ramakrishna\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\regexp.py:1199\u001b[0m, in \u001b[0;36mRegexpParser.__init__\u001b[1;34m(self, grammar, root_label, loop, trace)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop \u001b[38;5;241m=\u001b[39m loop\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grammar, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_grammar(grammar, root_label, trace)\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;66;03m# Make sur the grammar looks like it has the right type:\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m     type_err \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected string or list of RegexpChunkParsers \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the grammar.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1204\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ramakrishna\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\regexp.py:1240\u001b[0m, in \u001b[0;36mRegexpParser._read_grammar\u001b[1;34m(self, grammar, root_label, trace)\u001b[0m\n\u001b[0;32m   1237\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;66;03m# Add the rule\u001b[39;00m\n\u001b[1;32m-> 1240\u001b[0m     rules\u001b[38;5;241m.\u001b[39mappend(RegexpChunkRule\u001b[38;5;241m.\u001b[39mfromstring(line))\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;66;03m# Record the final stage\u001b[39;00m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_stage(rules, lhs, root_label, trace)\n",
      "File \u001b[1;32mc:\\Users\\Ramakrishna\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\regexp.py:395\u001b[0m, in \u001b[0;36mRegexpChunkRule.fromstring\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIllegal chunk pattern: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m rule)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, re\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIllegal chunk pattern: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m rule) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Illegal chunk pattern: {<P><NP>>}"
     ]
    }
   ],
   "source": [
    "#Example 2 \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sample =\"Mary saw a cat sitting on the mat \"\n",
    "pos = nltk.pos_tag(word_tokenize(sample))\n",
    "pos\n",
    "regx=r\"\"\"\n",
    "     NP: {<DT>?<JJ>*<NN>}\n",
    "     P: {<IN>}\n",
    "     V: {<V.*>}\n",
    "     PP: {<P><NP>>}\n",
    "     VP: {<VB.*><NP|PP>*}\n",
    "\"\"\"\n",
    "parser=nltk.RegexpParser(regx)\n",
    "tree= parser.parse(pos)\n",
    "print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
